{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e048b7",
   "metadata": {},
   "source": [
    "![](Bellman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727df31-35c8-4da9-8a57-2ccd29534a19",
   "metadata": {},
   "source": [
    "Q-Learning es un tipo de algoritmo de aprendizaje por refuerzo que le permite aprender a un \"agente\" a tomar decisiones óptimas y alcanzar un objetivo en ese entorno o ambiente determinado, el agente opera aprendiendo los valores que sean más convenientes para cada paso que tenga que dar, y a esos pasos los implementamos en un par de datos que definen la acción que tiene que tomar y el estado en el que queda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3cad85-73e3-4089-8929-586c2ec4ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d9a76f-7611-41b3-a22c-250a2a106b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensiones = (5, 5)\n",
    "estado_inicial = (0, 0)\n",
    "estado_objetivo = (4, 4)\n",
    "obstaculos = [(1, 1), (1, 3), (2, 3), (3, 0)]\n",
    "acciones = [(-1, 0), (1, 0), (0, -1), (0, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec6b130-fd52-4a9a-8d1d-be4d9c8bc5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_estados = dimensiones[0] * dimensiones[1]\n",
    "num_estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0addea6-aa2d-4056-aebd-c673a01a5e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_acciones = len(acciones)\n",
    "num_acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae311258-0445-4572-b3af-77fb063484a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((num_estados, num_acciones))\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f19a8ee-5796-4e79-845e-778db4758368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estado_a_indice(estado):\n",
    "    return estado[0] * dimensiones[1] + estado[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4d8df4-9a59-4396-80c1-117c85520647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ejemplo = estado_a_indice((1, 0))\n",
    "ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2dfb1f-77a6-42d9-9e94-8804bd047896",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "episodios = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe845cf-2568-48a5-84d8-e7d71c84d331",
   "metadata": {},
   "source": [
    "- **'alpha'**: controla cuanto se actualiza el valor Q en cada paso del aprendizaje, un valor de alpha más alto significa que la información más reciente tiene un peso mayor, y esto le permite un aprendizaje más rápido, pero también potencialmente menos estable.\n",
    "- **'gamma'**: es un factor de descuento, lo que hace es determinar la importancia de las recompensas que va a obtener en el futuro, si tenemos un valor de gamma cercano a 1 hace que las recompensas futuras sean casi tan importantes como las recompensas inmediatas y de esta manera incentiva a nuestro agente a considerar consecuencias a largo plazo de sus acciones.\n",
    "- **'epsilon'**: este valor sirve para que el agente no repita las mismas decisiones, definiendo la probabilidad de que el agente tome una acción aleatoria en lugar de la mejor decisión conocida hasta el momento según la tabla Q\n",
    "- **'episodios'**: número de veces que se estará repitiendo este proceso de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa758d17-2c1c-4a8a-b105-ee06fdf49f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elegir_accion(estado):\n",
    "    \"\"\"\n",
    "    Implementa la estrategia epsilon-greedy\n",
    "    Exploración: elige una acción aleatoria\n",
    "    Explotación: elige la mejor acción según la matriz Q\n",
    "    \"\"\"\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(range(num_acciones))\n",
    "    else:\n",
    "        return np.argmax(Q[estado_a_indice(estado)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3b3772d-febd-4af5-8a05-027d35a56142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_accion(estado, accion_idx):\n",
    "    \"\"\"\n",
    "    Aplica una acción basada en un índice de acción dado y actualiza el estado del agente en el grid.\n",
    "\n",
    "    Parámetros:\n",
    "    - estado (tuple): La posición actual del agente en el grid.\n",
    "    - accion_idx (int): El índice de la acción a aplicar, que está basado en la lista 'acciones'.\n",
    "\n",
    "    Retorna:\n",
    "    - tuple: El nuevo estado del agente después de aplicar la acción.\n",
    "    - int: La recompensa o penalización resultante de la acción.\n",
    "    - bool: Un booleano que indica si el objetivo ha sido alcanzado.\n",
    "    \"\"\"\n",
    "    accion = acciones[accion_idx]\n",
    "    nuevo_estado = tuple(np.add(estado, accion) % dimensiones)\n",
    "\n",
    "    if nuevo_estado in obstaculos or nuevo_estado == estado:\n",
    "        return estado, -100, False\n",
    "        \n",
    "    if nuevo_estado == estado_objetivo:\n",
    "        return nuevo_estado, 100, True\n",
    "        \n",
    "    return nuevo_estado, -1, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79b14eaa-d1d9-4e73-a509-687a36719f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episodio in range(episodios):\n",
    "    estado = estado_inicial\n",
    "    terminado = False\n",
    "\n",
    "    while not terminado:\n",
    "        idx_estado = estado_a_indice(estado)\n",
    "        accion_idx = elegir_accion(estado)\n",
    "        nuevo_estado, recompensa, terminado = aplicar_accion(estado, accion_idx)\n",
    "        idx_nuevo_estado = estado_a_indice(nuevo_estado)\n",
    "\n",
    "        Q[idx_estado, accion_idx] = Q[idx_estado, accion_idx] + alpha * (recompensa + gamma * np.max(Q[idx_nuevo_estado]) - Q[idx_estado, accion_idx])\n",
    "\n",
    "        estado = nuevo_estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3589372-f241-4430-8774-017bb3d0b5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politica = np.zeros(dimensiones, dtype=int)\n",
    "politica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "667c837b-bad6-4136-9221-d3311f52a31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politica aprendida (0: arriba, 1: abajo, 2: izquierda, 3: derecha\n",
      "[[2 0 2 3 0]\n",
      " [0 0 2 0 0]\n",
      " [3 2 0 0 0]\n",
      " [0 1 3 2 0]\n",
      " [2 2 3 1 0]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, dimensiones[0]):\n",
    "    for j in range(0, dimensiones[1]):\n",
    "        estado = (i, j)\n",
    "        idx_estado = estado_a_indice(estado)\n",
    "        mejor_accion = np.argmax(Q[idx_estado])\n",
    "        politica[i, j] = mejor_accion\n",
    "\n",
    "print('Politica aprendida (0: arriba, 1: abajo, 2: izquierda, 3: derecha')\n",
    "print(politica)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
